# coding: utf8
from fastapi import FastAPI, Body
from pydantic import BaseModel
import spacy
from spacy.matcher import Matcher
from typing import List, Dict

app = FastAPI(
    title="Matcher Service",
    description="Using FastAPI to reproduce matcher backend from <https://explosion.ai/demos/matcher>, based on <https://github.com/explosion/spacy-services>."
)

MODELS = {"en_core_web_sm": spacy.load("en_core_web_sm")}


class MatchData(BaseModel):
    text: str
    # Pattern format changed for spacy v3
    pattern: List[
        List[Dict[str, str]]
    ]


def get_model_desc(nlp, model_name):
    """Get human-readable model name, language name and version."""
    lang_cls = spacy.util.get_lang_class(nlp.lang)
    lang_name = lang_cls.__name__
    model_version = nlp.meta["version"]
    return f"{lang_name} - {model_name} (v{model_version})"


@app.get("/models")
def models():
    """Get human-readable model name, language name and version."""
    return {name: get_model_desc(nlp, name) for name, nlp in MODELS.items()}


@app.post("/match")
def match(
    model: str = "en_core_web_sm", 
    data: MatchData = Body(
        ...,  # required param
        example={
            "text": "A match is a tool for starting a fire. Typically, modern matches are made of small wooden sticks or stiff paper. One end is coated with a material that can be ignited by frictional heat generated by striking the match against a suitable surface. Wooden matches are packaged in matchboxes, and paper matches are partially cut into rows and stapled into matchbooks.",
            "pattern": [
                [{"POS": "ADJ"}, {"OP": "?"}],
                [{"LEMMA": "match"}, {"POS": "NOUN"}],
                [{"LEMMA": "be"}]
            ]
        }
    ),
):
    """Match text tokens based on input pattern"""
    nlp = MODELS[model]

    matcher = Matcher(nlp.vocab)
    matcher.add("PATTERN", data.pattern) # pattern args changed for v3
    
    doc = nlp(data.text)
    tokens = []
    matches = []
    match_tokens = set()

    for _, start, end in matcher(doc):
        if start >= end:  # filter out null matches or results of weird bug
            continue
        span = doc[start:end]
        if span[0].i in match_tokens:  # filter out overlaps
            continue
        match_tokens.update([t.i for t in span])
        matches.append(
            {"start": span.start_char, "end": span.end_char, "label": "MATCH"}
        )

    for t in doc:
        start = t.idx
        end = t.idx + len(t.text)
        label = "MATCH" if t.i in match_tokens else "TOKEN"
        tokens.append({"start": start, "end": end, "label": label})
    
    return {"matches": matches, "tokens": tokens}
